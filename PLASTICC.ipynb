{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "###IMPORT THE REQUIRED PACKAGES###\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom catboost import Pool, CatBoostClassifier\nimport itertools\nimport pickle, gzip\nimport glob\nfrom sklearn.preprocessing import StandardScaler\nfrom tsfresh.feature_extraction import extract_features\nimport sys, os\nimport argparse\nimport time\nfrom datetime import datetime as dt\nimport gc; gc.enable()\nfrom functools import partial, wraps\nnp.warnings.filterwarnings('ignore')\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom numba import jit", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "###FEATURE ENGINEERING FOR THE COORDINATES###\n\n@jit\ndef haversine_plus(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees) from \n    #https://stackoverflow.com/questions/4913349/haversine-formula\n    -in-python-bearing-and-distance-between-two-gps-points\n    \"\"\"\n    #Convert decimal degrees to Radians:\n    lon1 = np.radians(lon1)\n    lat1 = np.radians(lat1)\n    lon2 = np.radians(lon2)\n    lat2 = np.radians(lat2)\n\n    #Implementing Haversine Formula: \n    dlon = np.subtract(lon2, lon1)\n    dlat = np.subtract(lat2, lat1)\n\n    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n                        np.multiply(np.cos(lat1), \n                        np.multiply(np.cos(lat2), \n                        np.power(np.sin(np.divide(dlon, 2)), 2))))\n    \n    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n    return {\n        'haversine': haversine, \n        'latlon1': np.subtract(np.multiply(lon1, lat1), \n                               np.multiply(lon2, lat2)), \n   }\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "###FEATURE ENGINEERING FOR THE DIMENSIONS OF DATA#\n\n@jit\ndef process_flux(df):\n    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n\n    df_flux = pd.DataFrame({\n        'flux_ratio_sq': flux_ratio_sq, \n        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n        index=df.index)\n    \n    return pd.concat([df, df_flux], axis=1)\n\n\n@jit\ndef process_flux_agg(df):\n    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n    flux_diff = df['flux_max'].values - df['flux_min'].values\n    \n    df_flux_agg = pd.DataFrame({\n        'flux_w_mean': flux_w_mean,\n        'flux_diff1': flux_diff,\n        'flux_diff2': flux_diff / df['flux_mean'].values,       \n        'flux_diff3': flux_diff /flux_w_mean,\n        }, index=df.index)\n    \n    return pd.concat([df, df_flux_agg], axis=1)\n    \n\ndef featurize(df, df_meta, aggs, fcp, n_jobs=4):\n    \"\"\"\n    Extracting Features from train set\n    Features from olivier's kernel\n    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n    \"\"\"\n    \n    df = process_flux(df)\n\n    agg_df = df.groupby('object_id').agg(aggs)\n    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n    agg_df = process_flux_agg(agg_df) \n\n    # Add more features with\n    agg_df_ts_flux_passband = extract_features(df, \n                                               column_id='object_id', \n                                               column_sort='mjd', \n                                               column_kind='passband', \n                                               column_value='flux', \n                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n\n    agg_df_ts_flux = extract_features(df, \n                                      column_id='object_id', \n                                      column_value='flux', \n                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n\n    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n                                      column_id='object_id', \n                                      column_value='flux_by_flux_ratio_sq', \n                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n\n    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n    \n    df_det = df[df['detected']==1].copy()\n    agg_df_mjd = extract_features(df_det, \n                                  column_id='object_id', \n                                  column_value='mjd', \n                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n    \n    agg_df_ts_flux_passband.index.rename('object_id', inplace=True) \n    agg_df_ts_flux.index.rename('object_id', inplace=True) \n    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True) \n    agg_df_mjd.index.rename('object_id', inplace=True)      \n    agg_df_ts = pd.concat([agg_df, \n                           agg_df_ts_flux_passband, \n                           agg_df_ts_flux, \n                           agg_df_ts_flux_by_flux_ratio_sq, \n                           agg_df_mjd], axis=1).reset_index()\n    \n    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n    return result\n\n\ndef process_meta(df):\n    meta_df = df\n    \n    meta_dict = dict()\n    # distance\n    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, meta_df['gal_l'].values, meta_df['gal_b'].values))\n    \n    meta_dict['hostgal_photoz_certain'] = np.multiply(meta_df['hostgal_photoz'].values, np.exp(meta_df['hostgal_photoz_err'].values))\n    \n    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n    return meta_df\n\n\ndef multi_weighted_logloss(y_true, y_preds, classes, class_weights):\n    \"\"\"\n    refactor from\n    @author olivier https://www.kaggle.com/ogrellier\n    multi logloss for PLAsTiCC challenge\n    \"\"\"\n    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n    # Trasform y_true in dummies\n    y_ohe = pd.get_dummies(y_true)\n    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n    # Transform to log\n    y_p_log = np.log(y_p)\n    # Get the log for ones, .values is used to drop the index of DataFrames\n    # Exclude class 99 for now, since there is no class99 in the training set\n    # we gave a special process for that class\n    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n    # Get the number of positives for each class\n    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n    # Weight average and divide by the number of positives\n    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n    y_w = y_log_ones * class_arr / nb_pos\n\n    loss = - np.sum(y_w) / np.sum(class_arr)\n    return loss\n\n\ndef lgbm_multi_weighted_logloss(y_true, y_preds):\n    # Taken from Giba's topic : https://www.kaggle.com/titericz\n    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n\n    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n    return 'wloss', loss, False\n\n\ndef save_importances(importances_):\n    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n    plt.figure(figsize=(8, 12))\n    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n    plt.tight_layout()\n    plt.savefig('importances_{}.png'.format(dt.now().strftime('%Y-%m-%d-%H-%M')))\n    return importances_\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def lgbm_modeling_cross_validation(params,\n                                   full_train, \n                                   y, \n                                   classes, \n                                   class_weights, \n                                   nr_fold=5, \n                                   random_state=1):\n\n    # Compute weights\n    w = y.value_counts()\n    weights = {i : np.sum(w) / w[i] for i in w.index}\n\n    clfs = []\n    importances = pd.DataFrame()\n    folds = StratifiedKFold(n_splits=nr_fold, \n                            shuffle=True, \n                            random_state=random_state)\n    \n    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n        \n        clf = LGBMClassifier(**params)\n        print('no {}-fold start'.format(fold_ +1))\n        clf.fit(\n            trn_x, trn_y,\n            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n            eval_metric=lgbm_multi_weighted_logloss,\n            verbose=100,\n            early_stopping_rounds=50,\n            sample_weight=trn_y.map(weights)\n        )\n        clfs.append(clf)\n        \n        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n        print('no {}-fold ends with best iter: {}'.format(fold_+1, clf.best_iteration_))\n        print('no {}-fold loss: {}'.format(fold_ + 1, \n              multi_weighted_logloss(val_y, oof_preds[val_, :], \n                                     classes, class_weights)))\n    \n        imp_df = pd.DataFrame({\n                'feature': full_train.columns,\n                'gain': clf.feature_importances_,\n                'fold': [fold_ + 1] * len(full_train.columns),\n                })\n        importances = pd.concat([importances, imp_df], axis=0)\n        \n\n    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n                                   classes=classes, class_weights=class_weights)\n    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n    df_importances = save_importances(importances_=importances)\n    df_importances.to_csv('lgbm_importances_{}.csv'.format(dt.now().strftime('%Y-%m-%d-%H-%M')), index=False)\n    unique_y = np.unique(y)\n    class_map = dict()\n    for i,val in enumerate(unique_y):\n        class_map[val] = i\n    y_map = np.zeros((y.shape[0],))\n    y_map = np.array([class_map[val] for val in y])\n    cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n    np.set_printoptions(precision=2)\n    sample_sub = ss\n    class_names = list(sample_sub.columns[1:-1])\n    del sample_sub;gc.collect()\n    plt.figure(figsize=(12,12))\n    foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n                                title='Confusion matrix')\n    return clfs, score\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig('confusion_{}.png'.format(dt.now().strftime('%Y-%m-%d-%H-%M')))\n        \n    return", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean):\n    \n    # process all features\n    full_test = featurize(df_, meta_, \n                          featurize_configs['aggs'], \n                          featurize_configs['fcp'])\n    full_test.fillna(0, inplace=True)\n\n    # Make predictions\n    preds_ = None\n    for clf in clfs_:\n        if preds_ is None:\n            preds_ = clf.predict_proba(full_test[features])\n        else:\n            preds_ += clf.predict_proba(full_test[features])\n            \n    preds_ = preds_ / len(clfs_)\n\n    # Compute preds_99 as the proba of class not being any of the others\n   \n    preds_99 = np.ones(preds_.shape[0])\n    for i in range(preds_.shape[1]):\n        preds_99 *= (1 - preds_[:, i])\n\n    # Create DataFrame from predictions\n    preds_df_ = pd.DataFrame(preds_, \n                             columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n    preds_df_['object_id'] = full_test['object_id']\n    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n    return preds_df_\n\n\ndef process_test(clfs, \n                 features, \n                 featurize_configs, \n                 train_mean,\n                 filename='predictions.csv'):\n    start = time.time()\n\n    #features pertaining to astrophysical dynamics\n    meta_test = process_meta(tester_meta)\n    meta_test['RMSE'] = (meta_test['hostgal_photoz'] - meta_test['hostgal_specz'])/(1 + meta_test['hostgal_specz'])\n    meta_test['gold_sample'] = meta_test['hostgal_photoz_err'] / (1 + meta_test['hostgal_photoz'])\n    \n    df = tester\n    preds_df = predict_chunk(df_=df,\n                             clfs_=clfs,\n                             meta_=meta_test,\n                             features=features,\n                             featurize_configs=featurize_configs,\n                             train_mean=train_mean)\n    \n    preds_df.to_csv(filename, header=True, mode='a', index=False)\n    \n    del preds_df\n    gc.collect()\n          \n    return", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n\nclient_223a804434a84be0af24c32f365848e5 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='<api-key>',\n    ibm_auth_endpoint=\"<end-point>\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_223a804434a84be0af24c32f365848e5.get_object(Bucket='hydrogen-donotdelete-pr-hcq3qswwybfps6',Key='training_set.csv')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ntrainer = pd.read_csv(body)\n\n\nbody = client_223a804434a84be0af24c32f365848e5.get_object(Bucket='hydrogen-donotdelete-pr-hcq3qswwybfps6',Key='training_set_metadata.csv')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ntrain_meta = pd.read_csv(body)\n\nbody = client_223a804434a84be0af24c32f365848e5.get_object(Bucket='hydrogen-donotdelete-pr-hcq3qswwybfps6',Key='tester_meta.csv')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ntester_meta = pd.read_csv(body)\n\nbody = client_223a804434a84be0af24c32f365848e5.get_object(Bucket='hydrogen-donotdelete-pr-hcq3qswwybfps6',Key='sample_submission.csv')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nss = pd.read_csv(body)\n\n\nbody = client_223a804434a84be0af24c32f365848e5.get_object(Bucket='hydrogen-donotdelete-pr-hcq3qswwybfps6',Key='tester.csv')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\ntester = pd.read_csv(body)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def main(argc, argv):\n    # Features to compute with tsfresh library. Fft coefficient is meant to capture periodicity    \n    \n    # agg features\n    aggs = {\n        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'detected': ['mean'],\n        'flux_ratio_sq':['sum', 'skew'],\n        'flux_by_flux_ratio_sq':['sum','skew'],\n    }\n    \n    # tsfresh features\n    fcp = {\n        'flux': {\n            'longest_strike_above_mean': None,\n            'longest_strike_below_mean': None,\n            'mean_change': None,\n            'mean_abs_change': None,\n            'length': None,\n        },\n                \n        'flux_by_flux_ratio_sq': {\n            'longest_strike_above_mean': None,\n            'longest_strike_below_mean': None,       \n        },\n                \n        'flux_passband': {\n            'fft_coefficient': [\n                    {'coeff': 0, 'attr': 'abs'}, \n                    {'coeff': 1, 'attr': 'abs'}\n                ],\n            'kurtosis' : None, \n            'skewness' : None,\n        },\n                \n        'mjd': {\n            'maximum': None, \n            'minimum': None,\n            'mean_change': None,\n            'mean_abs_change': None,\n        },\n    }\n\n    best_params = {\n            'device': 'cpu', \n            'objective': 'multiclass', \n            'num_class': 14, \n            #'boosting_type': 'dart','gbdt' \n            'boosting_type': 'dart', \n            'n_jobs': -1, \n            'max_depth': 12,\n            'n_estimators': 500, \n            'subsample_freq': 20, \n            'subsample_for_bin': 5000, \n            'min_data_per_group': 100, \n            'max_cat_to_onehot': 4, \n            'cat_l2': 1.0, \n            'cat_smooth': 59.5, \n            'max_cat_threshold': 32, \n            'metric_freq': 10, \n            'verbosity': -1, \n            'metric': 'multi_logloss', \n            'uniform_drop': True, \n            'colsample_bytree': 0.5, \n            'drop_rate': 0.173, \n            'learning_rate': 0.0267, \n            'max_drop': 5, \n            'min_child_samples': 10, \n            'min_child_weight': 100.0, \n            'min_split_gain': 0.1, \n            'num_leaves': 7, \n            'reg_alpha': 0.1, \n            'reg_lambda': 0.00023, \n            'skip_drop': 0.44, \n            'subsample': 0.75}\n\n    meta_train = process_meta(train_meta)\n    \n    #features pertaining to astrophysical dynamics\n    meta_train['RMSE'] = (meta_train['hostgal_photoz'] - meta_train['hostgal_specz'])/(1 + meta_train['hostgal_specz'])\n    meta_train['gold_sample'] = meta_train['hostgal_photoz_err'] / (1 + meta_train['hostgal_photoz'])\n\n    train = trainer\n    full_train = featurize(train, meta_train, aggs, fcp)\n\n    if 'target' in full_train:\n        y = full_train['target']\n        del full_train['target']\n        \n    classes = sorted(y.unique())    \n    # Taken from Giba's topic : https://www.kaggle.com/titericz\n    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n    class_weights = {c: 1 for c in classes}\n    class_weights.update({c:2 for c in [64, 15]})\n    print('Unique classes : {}, {}'.format(len(classes), classes))\n    print(class_weights)\n   \n    \n    if 'object_id' in full_train:\n        oof_df = full_train[['object_id']]\n        del full_train['object_id'] \n        #del full_train['distmod'] \n        del full_train['hostgal_specz']\n        del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n        del full_train['ddf']\n    \n    train_mean = full_train.mean(axis=0)\n    \n    pd.set_option('display.max_rows', 500)\n    print(full_train.describe().T)\n    \n    full_train.fillna(0, inplace=True)\n\n    eval_func = partial(lgbm_modeling_cross_validation, \n                        full_train=full_train, \n                        y=y, \n                        classes=classes, \n                        class_weights=class_weights, \n                        nr_fold=15, \n                        random_state=1)\n\n    best_params.update({'n_estimators': 1000})\n    \n    # modeling from CV\n    clfs, score = eval_func(best_params)\n    \n    \n\n    filename = 'subm_{:.6f}_{}.csv'.format(score, \n                     dt.now().strftime('%Y-%m-%d-%H-%M'))\n    print('save to {}'.format(filename))\n    # TEST\n    process_test(clfs, \n                 features=full_train.columns, \n                 featurize_configs={'aggs': aggs, 'fcp': fcp}, \n                 train_mean=train_mean, \n                 filename=filename)\n        \n    z = pd.read_csv(filename)\n    print(\"Shape BEFORE grouping: {}\".format(z.shape))\n    z = z.groupby('object_id').mean()\n    print(\"Shape AFTER grouping: {}\".format(z.shape))\n    z.to_csv('single_{}'.format(filename), index=True)\n\n\nif __name__ == '__main__':\n    main(len(sys.argv), sys.argv)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}